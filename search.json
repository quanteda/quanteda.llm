[{"path":"https://quanteda.github.io/quanteda.llm/articles/getting-started.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting started with quanteda.llm","text":"vignette provides brief introduction quanteda.llm package, designed facilitate use large language models (LLMs) text analysis workflows. package integrates quanteda framework, allowing users leverage LLMs various text processing tasks. package relies ellmer package LLM interactions, providing seamless interface users work different LLM providers. information ellmer package supported LLM interactions, please refer documentation .","code":""},{"path":"https://quanteda.github.io/quanteda.llm/articles/getting-started.html","id":"basic-usage","dir":"Articles","previous_headings":"","what":"Basic usage","title":"Getting started with quanteda.llm","text":"get started quanteda.llm, first need install package GitHub. , can load package begin using functions.","code":"library(quanteda.llm) #> Loading required package: ellmer"},{"path":"https://quanteda.github.io/quanteda.llm/articles/getting-started.html","id":"analyzing-texts","dir":"Articles","previous_headings":"","what":"Analyzing texts","title":"Getting started with quanteda.llm","text":"quanteda.llm package provides functions analyze large amounts texts using LLMs. similar manual annotations, automates process using LLMs. package includes functions summarization, salience rating, scaling, text analysis tasks.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/articles/getting-started.html","id":"structuring-llm-responses","dir":"Articles","previous_headings":"","what":"Structuring LLM responses","title":"Getting started with quanteda.llm","text":"package allows structure responses LLMs way compatible quanteda’s corpus principles useful common text analysis tasks. means can easily integrate LLM-generated data text analysis workflows. example, can ask LLM summarize documents corpus (ai_summary()) store summaries document variables, can classify documents topics (ai_salience()) scale based predefined criteria (ai_scale()) store results document variables. need flexibility LLM generates output, can use ai_text() function define custom prompts response structures. ai_text() help type_object() argument ellmer package, can define LLM format output, specifying fields include response format response . flexibility enables tailor LLM’s output analysis requirements, making easier integrate LLM-generated data text analysis workflows.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/articles/getting-started.html","id":"example-uses","dir":"Articles","previous_headings":"","what":"Example uses","title":"Getting started with quanteda.llm","text":"vignette provides brief overview use quanteda.llm package analyzing texts LLMs. following examples demonstrate summarize documents, classify topics, score documents using LLMs. end, also show validate LLM responses manually customize structure LLM responses advanced text analysis tasks. detailed examples including code snippets, please refer section Examples.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/articles/getting-started.html","id":"summarizing-documents","dir":"Articles","previous_headings":"Example uses","what":"Summarizing documents","title":"Getting started with quanteda.llm","text":"ai_summary() function allows summarize documents using LLM. generates summary document character vector stores new character vector can added document variable quanteda corpus element. function uses predefined type_object argument ellmer structure LLM’s response, producing succinct summaries document. Users need provide character vector documents summarize choose LLM provider want use summarization.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/articles/getting-started.html","id":"salience-rating-of-topics-in-documents","dir":"Articles","previous_headings":"Example uses","what":"Salience rating of topics in documents","title":"Getting started with quanteda.llm","text":"ai_salience() function allows classify documents based relevance predefined topics. function uses predefined type_object argument ellmer structure LLM’s response, producing list topics salience scores document. function particularly useful analyzing large corpora manual classification impractical. Users need provide character vector documents list topics classify. LLM analyze document assign salience score topic, indicating relevant document topic.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/articles/getting-started.html","id":"scoring-documents-on-a-predefined-scale","dir":"Articles","previous_headings":"Example uses","what":"Scoring documents on a predefined scale","title":"Getting started with quanteda.llm","text":"ai_score() function allows score documents based predefined scale. function uses predefined type_object argument ellmer structure LLM’s response, producing score document based specified scale well short justification score. function useful evaluating documents specific criteria benchmarks. Users need provide character vector documents scale score . LLM analyze document assign score based provided scale, along brief explanation reasoning behind score.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/articles/getting-started.html","id":"manually-checking-and-validating-llm-responses","dir":"Articles","previous_headings":"Example uses","what":"Manually checking and validating LLM responses","title":"Getting started with quanteda.llm","text":"ai_validate() function allows users manually check validate responses generated LLM user-friendly ShinyApp. manual checks essential ensuring quality accuracy LLM’s output. function can used review scores justifications generated LLM, users can also highlight save examples original texts support validated text classifications. saved examples can used qualitative analyses built labelled dataset fine-tuning open-source LLMs receive improved performance similar tasks.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/articles/getting-started.html","id":"customizing-the-structure-of-llm-responses","dir":"Articles","previous_headings":"Example uses","what":"Customizing the structure of LLM responses","title":"Getting started with quanteda.llm","text":"quanteda.llm package allows customize structure LLM responses fit specific analysis needs. can define LLM format output, specifying fields include response format response . flexibility enables tailor LLM’s output analysis requirements, making easier integrate LLM-generated data text analysis workflows. advanced text analysis tasks, can use ai_text() function define custom prompts response structures. function allows specify LLM generate output, including format content response. using type_object() ellmer package, can define structure LLM’s response, making easier integrate LLM-generated data text analysis workflows.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/articles/getting-started.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Getting started with quanteda.llm","text":"quanteda.llm package provides powerful flexible framework integrating large language models text analysis workflows. leveraging LLMs, users can automate various text processing tasks, summarization, classification, scoring, maintaining compatibility quanteda framework. package’s ability structure LLM responses customize output formats makes valuable tool researchers analysts working large text corpora.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/articles/pkgdown/examples/salience.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Salience rating of topics","text":"","code":"library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quanteda.llm) ## Loading required package: ellmer library(quanteda.tidy) ##  ## Attaching package: 'quanteda.tidy' ## The following object is masked from 'package:stats': ##  ##     filter data_corpus_inaugural <- tail(data_corpus_inaugural, 3)"},{"path":"https://quanteda.github.io/quanteda.llm/articles/pkgdown/examples/salience.html","id":"using-ai_salience-for-salience-rating-of-topics","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using ai_salience() for salience rating of topics","title":"Example: Salience rating of topics","text":"","code":"# define the topics for salience classification topics <- c(\"economy\", \"environment\", \"healthcare\") data_corpus_inaugural <- data_corpus_inaugural %>%   mutate(ai_salience(text, topics, chat_fn = chat_openai, model = \"gpt-4o\",                      api_args = list(temperature = 0, seed = 42))) ##  ## Calling chat_fn (gpt-4o): ## ... processing: [1/3] ## ... processing: [2/3] ## ... processing: [3/3] ## Finished. # topic and score are created as new docvars in the corpus glimpse(data_corpus_inaugural) ## Rows: 3 ## Columns: 13 ## $ doc_id    <chr> \"2017-Trump\", \"2021-Biden\", \"2025-Trump\" ## $ text      <chr> \"Chief Just…\", \"Chief Just…\", \"Thank you.…\" ## $ Year      <int> 2017, 2021, 2025 ## $ President <chr> \"Trump\", \"Biden\", \"Trump\" ## $ FirstName <chr> \"Donald J.\", \"Joseph R.\", \"Donald J.\" ## $ Party     <fct> Republican, Democratic, Republican ## $ id        <chr> \"1\", \"2\", \"3\" ## $ topic1    <dbl> 1, 1, 1 ## $ topic2    <dbl> 2, 2, 2 ## $ topic3    <dbl> 3, 3, 3 ## $ score1    <dbl> 0.6, 0.2, 0.5 ## $ score2    <dbl> 0.1, 0.2, 0.3 ## $ score3    <dbl> 0.3, 0.6, 0.2"},{"path":"https://quanteda.github.io/quanteda.llm/articles/pkgdown/examples/scoring.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Scaling texts","text":"","code":"library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quanteda.llm) ## Loading required package: ellmer library(quanteda.tidy) ##  ## Attaching package: 'quanteda.tidy' ## The following object is masked from 'package:stats': ##  ##     filter data_corpus_inaugural <- tail(data_corpus_inaugural, 3)"},{"path":"https://quanteda.github.io/quanteda.llm/articles/pkgdown/examples/scoring.html","id":"using-ai_score-for-scoring-documents","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using ai_score() for scoring documents","title":"Example: Scaling texts","text":"","code":"prompt <- \"Score the following document on a scale of how much it aligns  with the political left. The political left is defined as groups  which advocate for social equality, government intervention in the  economy, and progressive policies. Use the following metrics:   SCORING METRIC:  3 : extremely left  2 : very left  1 : slightly left  0 : not at all left\"  data_corpus_inaugural <- data_corpus_inaugural %>%   mutate(ai_score(text, prompt, chat_fn = chat_openai, model = \"gpt-4o\",                   api_args = list(temperature = 0, seed = 42))) ##  ## Calling chat_fn (gpt-4o): ## ... processing: [1/3] ## ... processing: [2/3] ## ... processing: [3/3] ## Finished. # score and evidence are created as new docvars in the corpus library(kableExtra) cbind(data.frame(docnames = docnames(data_corpus_inaugural)),        select(docvars(data_corpus_inaugural), score, evidence)) %>%   kable(\"html\", escape = FALSE) %>%   kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %>%   column_spec(3)"},{"path":"https://quanteda.github.io/quanteda.llm/articles/pkgdown/examples/structuring.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Structuring LLM responses for text analysis","text":"","code":"library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quanteda.llm) ## Loading required package: ellmer library(quanteda.tidy) ##  ## Attaching package: 'quanteda.tidy' ## The following object is masked from 'package:stats': ##  ##     filter data_corpus_inaugural <- tail(data_corpus_inaugural, 3)"},{"path":"https://quanteda.github.io/quanteda.llm/articles/pkgdown/examples/structuring.html","id":"using-ai_text-for-scoring-documents","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using ai_text() for scoring documents","title":"Example: Structuring LLM responses for text analysis","text":"","code":"prompt <- \"Score the following document on a scale of how much it aligns with the political left. The political left is defined as groups which advocate for social equality, government intervention in the economy, and progressive policies. Use the following metrics: SCORING METRIC: 3 : extremely left 2 : very left 1 : slightly left 0 : not at all left\"  # define the structure of the response policy_scores <- type_object(   score = type_integer(),   evidence = type_string())  data_corpus_inaugural <- data_corpus_inaugural %>%   mutate(ai_text(text, chat_fn = chat_openai, model = \"gpt-4o\", type_object = policy_scores,                  system_prompt = prompt,                   api_args = list(temperature = 0, seed = 42))) ##  ## Calling chat_openai (gpt-4o): ## ... processing: [1/3] ## ... processing: [2/3] ## ... processing: [3/3] ## Finished. # score and evidence are created as new docvars in the corpus library(kableExtra) cbind(data.frame(docnames = docnames(data_corpus_inaugural)),        select(docvars(data_corpus_inaugural), score, evidence)) %>%   kable(\"html\", escape = FALSE) %>%   kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %>%   column_spec(3)"},{"path":"https://quanteda.github.io/quanteda.llm/articles/pkgdown/examples/summarizing.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Summarizing texts","text":"","code":"library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quanteda.llm) ## Loading required package: ellmer library(quanteda.tidy) ##  ## Attaching package: 'quanteda.tidy' ## The following object is masked from 'package:stats': ##  ##     filter data_corpus_inaugural <- tail(data_corpus_inaugural, 3)"},{"path":"https://quanteda.github.io/quanteda.llm/articles/pkgdown/examples/summarizing.html","id":"using-ai_summary-for-summarization-of-documents","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using ai_summary() for summarization of documents","title":"Example: Summarizing texts","text":"","code":"# creates a new docvar \"summary\" data_corpus_inaugural <- data_corpus_inaugural %>%   mutate(ai_summary(text, chat_fn = chat_openai, model = \"gpt-4o\",                     api_args = list(temperature = 0, seed = 42))) ##  ## Calling chat_fn (gpt-4o): ## ... processing: [1/3] ## ... processing: [2/3] ## ... processing: [3/3] ## Finished. library(kableExtra) cbind(data.frame(docnames = docnames(data_corpus_inaugural)),        summary = data_corpus_inaugural$summary) %>%   kable(\"html\", escape = FALSE) %>%   kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %>%   column_spec(2)"},{"path":"https://quanteda.github.io/quanteda.llm/articles/pkgdown/examples/validating.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Manually validating LLM responses","text":"","code":"library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quanteda.llm) ## Loading required package: ellmer library(quanteda.tidy) ##  ## Attaching package: 'quanteda.tidy' ## The following object is masked from 'package:stats': ##  ##     filter data_corpus_inaugural <- tail(data_corpus_inaugural, 3)"},{"path":"https://quanteda.github.io/quanteda.llm/articles/pkgdown/examples/validating.html","id":"using-ai_validate-to-manually-check-llm-generated-outputs","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using ai_validate() to manually check LLM-generated outputs","title":"Example: Manually validating LLM responses","text":"","code":"data_corpus_inaugural <- data_corpus_inaugural %>%   mutate(ai_validate(text, score, evidence)) # validated scores and comments as well as highlighted examples from texts are # created as new docvars in the corpus"},{"path":"https://quanteda.github.io/quanteda.llm/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Seraphine F. Maerz. Author, maintainer. Kenneth Benoit. Author.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Maerz S, Benoit K (2025). quanteda.llm: LLM support quanteda. R package version 0.4.2, https://github.com/quanteda/quanteda.llm.","code":"@Manual{,   title = {quanteda.llm: LLM support for quanteda},   author = {Seraphine F. Maerz and Kenneth Benoit},   year = {2025},   note = {R package version 0.4.2},   url = {https://github.com/quanteda/quanteda.llm}, }"},{"path":"https://quanteda.github.io/quanteda.llm/index.html","id":"quantedallm","dir":"","previous_headings":"","what":"LLM support for quanteda","title":"LLM support for quanteda","text":"quanteda.llm package makes easy use LLMs quanteda corpora (character vectors data frames), enable classification, summarisation, scoring, analysis documents text. quanteda provides host convenient functions managing, manipulating, describing corpora well linking document variables metadata documents. quanteda.llm makes convenient link LLMs analysing classifying texts, creating new variables created LLMs. Using tidy approach linking new quanteda.tidy package, enable convenient operations using common Tidyverse functions manipulating LLM-created objects variables.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/index.html","id":"included-functions","dir":"","previous_headings":"","what":"Included functions","title":"LLM support for quanteda","text":"package includes following functions: generic function can used LLM supported ellmer. Generates structured responses classifications based pre-defined instructions texts quanteda corpus. Users can flexibly define prompts structure responses via type_object() ellmer package. Users can add dataset examples improve LLM performance (-shot prompting) Supports resuming interrupted processes result_env environment. Starts interactive app manually validate LLM-generated outputs. Allows users review validate LLM-generated outputs justifications, marking valid invalid. Supports resuming validation process case interruptions result_env environment. wrapper around ai_text() summarizing documents corpus. Uses pre-defined type_object() structure summary output. wrapper around ai_text() computing salience scores topics corpus. Uses pre-defined type_object() structure salience classification output. wrapper around ai_text() scoring documents based scale defined prompt. Uses pre-defined type_object() structure scoring output.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/index.html","id":"supported-llms","dir":"","previous_headings":"","what":"Supported LLMs","title":"LLM support for quanteda","text":"package supports LLMs currently available ellmer package, including: Anthropic’s Claude: chat_anthropic(). AWS Bedrock: chat_aws_bedrock(). Azure OpenAI: chat_azure_openai(). Cloudflare: chat_cloudflare(). Databricks: chat_databricks(). DeepSeek: chat_deepseek(). GitHub model marketplace: chat_github(). Google Gemini/Vertex AI: chat_google_gemini(), chat_google_vertex(). Groq: chat_groq(). Hugging Face: chat_huggingface(). Mistral: chat_mistral(). Ollama: chat_ollama(). OpenAI: chat_openai(). OpenRouter: chat_openrouter(). perplexity.ai: chat_perplexity(). Snowflake Cortex: chat_snowflake() chat_cortex_analyst(). VLLM: chat_vllm(). authentication usage LLMs, please refer respective ellmer documentation . example, use chat_openai models, need sign API key OpenAI can save .Renviron file OPENAI_API_KEY. use chat_ollama models, first download install Ollama. install models either command line (e.g. ollama pull llama3.1) within R using rollama package. Ollama app must running models used.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"LLM support for quanteda","text":"can install development version quanteda.llm https://github.com/quanteda/quanteda.llm :","code":"# install.packages(\"pak\") pak::pak(\"quanteda/quanteda.llm\") pak::pak(\"quanteda/quanteda.tidy\")"},{"path":"https://quanteda.github.io/quanteda.llm/index.html","id":"example-use","dir":"","previous_headings":"","what":"Example use","title":"LLM support for quanteda","text":"learn use package, please refer following examples: Summarizing texts LLMs Salience ratings topics LLMs Scoring texts LLMs Structuring LLM responses text analysis Validating LLM responses","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_salience.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute salience scores for topics — ai_salience","title":"Compute salience scores for topics — ai_salience","text":"function wrapper around ai_text() compute salience scores topics corpus using chat function.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_salience.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute salience scores for topics — ai_salience","text":"","code":"ai_salience(.data, topics, chat_fn, ..., verbose = TRUE)"},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_salience.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute salience scores for topics — ai_salience","text":".data character quanteda::corpus object containing documents summarized topics character; vector topic names salience scores computed; scores sum 1 across topics chat_fn function; chat function ellmer ... additional arguments passed chat_fn verbose logical; output progress indicator TRUE","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_salience.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute salience scores for topics — ai_salience","text":"character; response LLM length equal number input documents, elements named input element names","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_salience.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute salience scores for topics — ai_salience","text":"","code":"if (FALSE) { # \\dontrun{ library(quanteda) salience <- ai_salience(data_corpus_inaugural[1:3],                         chat_fn = chat_openai, model = \"gpt-4o\",                         api_args = list(temperature = 0, seed = 42),                         topics = c(\"economy\", \"environment\", \"healthcare\")) } # }"},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_score.html","id":null,"dir":"Reference","previous_headings":"","what":"Score documents on pre-defined scales — ai_score","title":"Score documents on pre-defined scales — ai_score","text":"function wrapper around ai_text compute scores documents using chat function.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_score.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Score documents on pre-defined scales — ai_score","text":"","code":"ai_score(.data, prompt, chat_fn, ..., verbose = TRUE)"},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_score.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Score documents on pre-defined scales — ai_score","text":".data character quanteda::corpus object containing documents summarized prompt character; prompt defines scoring criteria chat_fn function; chat function ellmer ... additional arguments passed chat_fn verbose logical; output progress indicator TRUE","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_score.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Score documents on pre-defined scales — ai_score","text":"character; response LLM length equal number input documents, elements named input element names","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_score.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Score documents on pre-defined scales — ai_score","text":"","code":"if (FALSE) { # \\dontrun{ library(quanteda) library(ellmer) # define a prompt for scoring documents prompt <- \"Score the following document on a scale of how much it aligns with the political left. The political left is defined as groups which advocate for social equality, government intervention in the economy, and progressive policies. Use the following metrics: SCORING METRIC: 3 : extremely left 2 : very left 1 : slightly left 0 : not at all left\" # compute scores for documents in the inaugural corpus scores <- ai_score(data_corpus_inaugural[1:3], prompt,                    chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0, seed = 42)) } # }"},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_summary.html","id":null,"dir":"Reference","previous_headings":"","what":"Summarize documents — ai_summary","title":"Summarize documents — ai_summary","text":"convenience wrapper around ai_text() summarize documents using chat function.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_summary.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summarize documents — ai_summary","text":"","code":"ai_summary(.data, chat_fn, ..., verbose = TRUE)"},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_summary.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summarize documents — ai_summary","text":".data character quanteda::corpus object containing documents summarized chat_fn function; chat function ellmer ... additional arguments passed chat_fn verbose logical; output progress indicator TRUE","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_summary.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summarize documents — ai_summary","text":"character; response LLM length equal number input documents, elements named input element names","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_summary.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Summarize documents — ai_summary","text":"","code":"if (FALSE) { # \\dontrun{ library(quanteda) library(ellmer) summaries <- ai_summary(data_corpus_inaugural[1:3],                         chat_fn = chat_openai, model = \"gpt-4o\",                         api_args = list(temperature = 0, seed = 42)) } # }"},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_text.html","id":null,"dir":"Reference","previous_headings":"","what":"Structured AI analysis of texts — ai_text","title":"Structured AI analysis of texts — ai_text","text":"function applies AI-assisted analysis document character vector docvar corpus, using structured type_object() define expected response format. supports -shot learning examples better performance.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_text.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Structured AI analysis of texts — ai_text","text":"","code":"ai_text(   .data,   chat_fn,   type_object,   few_shot_examples = NULL,   verbose = TRUE,   result_env = NULL,   ... )"},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_text.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Structured AI analysis of texts — ai_text","text":".data character vector texts chat_fn function like chat_openai(). See https://ellmer.tidyverse.org/articles/structured-data.html details. type_object ellmer type_object() few_shot_examples Optional -shot learning examples (data frame text, score) verbose logical; whether print progress result_env environment store results allow resuming ... additional arguments passed chat_fn","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_text.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Structured AI analysis of texts — ai_text","text":"character; response LLM length equal number input documents; single element defined type_object() added character vector","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_text.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Structured AI analysis of texts — ai_text","text":"","code":"if (FALSE) { # \\dontrun{ library(ellmer) results <- quanteda::data_corpus_inaugural[1:3] %>%   ai_text(chat_fn = chat_openai, model = \"gpt-4o\",           api_args = list(temperature = 0, seed = 42),           type_object =             type_object(\"Summary of the document\",                         summary = type_string(\"Summarize the document in a few sentences.\")),   ) } # }"},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_validate.html","id":null,"dir":"Reference","previous_headings":"","what":"Starts an interactive app to manually validate the output of an LLM stored in a character vector — ai_validate","title":"Starts an interactive app to manually validate the output of an LLM stored in a character vector — ai_validate","text":"function launches Shiny app allows users manually validate output LLM analysis, ai_score comments manual validation text added additional docvar input data texts yet validated NA.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_validate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Starts an interactive app to manually validate the output of an LLM stored in a character vector — ai_validate","text":"","code":"ai_validate(   text,   llm_output,   llm_evidence = NULL,   result_env = new.env(),   ...,   verbose = TRUE,   launch_app = TRUE )"},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_validate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Starts an interactive app to manually validate the output of an LLM stored in a character vector — ai_validate","text":"text character quanteda::corpus object containing documents manually validated llm_output character string; name LLM output column contains summaries, labels, scores validated llm_evidence character vector; name additional LLM output evidence justifications provided LLM result_env environment store results allow resuming ... additional arguments passed chat_fn verbose logical; output progress indicator TRUE launch_app Logical, whether launch interactive Shiny app. Defaults TRUE.","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_validate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Starts an interactive app to manually validate the output of an LLM stored in a character vector — ai_validate","text":"character; response manual validation length equal number input documents, elements named input element names","code":""},{"path":"https://quanteda.github.io/quanteda.llm/reference/ai_validate.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Starts an interactive app to manually validate the output of an LLM stored in a character vector — ai_validate","text":"","code":"if (FALSE) { # \\dontrun{ library(quanteda) summ1 <- ai_summarize(data_char_ukimmig2010, chat_fn = chat_ollama, model = \"llama3.2\") summ2 <- ai_summarize(data_corpus_inaugural[1:2], chat_fn = chat_openai,                 api_args = list(temperature = 0, seed = 42)) validate1 <- ai_validate(data_char_ukimmig2010, llm_output = summ1, verbose = TRUE) validate2 <- ai_validate(data_corpus_inaugural[1:2], llm_output = summ2, verbose = TRUE) } # }"},{"path":"https://quanteda.github.io/quanteda.llm/reference/quanteda.llm-package.html","id":null,"dir":"Reference","previous_headings":"","what":"quanteda.llm: LLM support for quanteda — quanteda.llm-package","title":"quanteda.llm: LLM support for quanteda — quanteda.llm-package","text":"Support linking LLM analysis quanteda.","code":""},{"path":[]},{"path":"https://quanteda.github.io/quanteda.llm/reference/quanteda.llm-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"quanteda.llm: LLM support for quanteda — quanteda.llm-package","text":"Maintainer: Seraphine F. Maerz seraphine.maerz@unimelb.edu.au (ORCID) Authors: Kenneth Benoit kbenoit@smu.edu.sg (ORCID)","code":""}]
