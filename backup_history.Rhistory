})
library(testthat)
test_that("ai_salience returns expected output for one document", {
chat_fn <- make_mock_chat_fn(c(0.95))
result <- ai_salience("Test document", topics = c("TopicA"), chat_fn = chat_fn, verbose = FALSE)
expect_s3_class(result, "data.frame")
expect_true("salience_TopicA" %in% colnames(result))
expect_true(is.numeric(result$salience_TopicA))
expect_equal(result$salience_TopicA, 0.95)
})
test_that("ai_summary returns simple structured summary", {
chat_fn <- function(...) {
structure(list(
get_model = function() "mock-model",
chat_structured = function(text, type) {
list(summary = paste("Summary of:", text))
}
), class = "MockChat")
}
result <- ai_summary("This is a test document.", chat_fn = chat_fn, verbose = FALSE)
expect_s3_class(result, "data.frame")
expect_equal(result$summary, "Summary of: This is a test document.")
})
summaries <- ai_summary(data_corpus_inaugural[1:3],
chat_fn = chat_openai, model = "gpt-4o",
api_args = list(temperature = 0, seed = 42))
summaries <- ai_summary(data_corpus_inaugural[1:3],
chat_fn = chat_openai, model = "gpt-4o",
api_args = list(temperature = 0, seed = 42))
summaries
test_that("ai_salience returns expected output for one document", {
chat_fn <- make_mock_chat_fn(c(0.95))
result <- ai_salience("Test document", topics = c("TopicA"), chat_fn = chat_fn, verbose = FALSE)
expect_s3_class(result, "data.frame")
expect_true("salience_TopicA" %in% colnames(result))
expect_true(is.numeric(result$salience_TopicA))
expect_equal(result$salience_TopicA, 0.95)
})
test_that("ai_summary returns simple structured summary", {
chat_fn <- function(...) {
structure(list(
get_model = function() "mock-model",
chat_structured = function(text, type) {
list(summary = paste("Summary of:", text))
}
), class = "MockChat")
}
result <- ai_summary("This is a test document.", chat_fn = chat_fn, verbose = FALSE)
expect_s3_class(result, "data.frame")
expect_equal(result$summary, "Summary of: This is a test document.")
})
result
expect_s3_class(result, "data.frame")
expect_equal(result$summary, "Summary of: This is a test document.")
result$summary
result
chat_fn <- function(...) {
structure(list(
get_model = function() "mock-model",
chat_structured = function(text, type) {
list(summary = paste("Summary of:", text))
}
), class = "MockChat")
}
result <- ai_summary("This is a test document.", chat_fn = chat_fn, verbose = FALSE)
result
chat_fn <- function(...) {
structure(list(
get_model = function() "mock-model",
chat_structured = function(text, type) {
list(summary = paste("Summary of:", text))
}
), class = "MockChat")
}
result <- ai_summary("This is a test document.", chat_fn = chat_fn, verbose = FALSE)
expect_s3_class(result, "data.frame")
result
result <- ai_summary("This is a test document.", chat_fn = chat_fn, verbose = FALSE)
result
llm_output <- readRDS("../test_data/ai_summary_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_1-3.rds")
llm_output
llm_output <- readRDS("../test_data/ai_summary_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_1-3.rds")
expect_s3_class(result, "data.frame")
?expect_s3_class
str(llm_output
)
expect_s3_class(llm_output, "data.frame")
llm_output$summary
llm_output
expect_identical(llm_output$id, names(txt))
test_that("ai_summary skips already processed documents", {
skip()
chat_fn <- function(...) {
structure(list(
get_model = function() "mock-model",
chat_structured = function(text, type) {
list(summary = paste("Summary of:", text))
}
), class = "MockChat")
}
docs <- c(doc1 = "Already processed.", doc2 = "To be processed.")
result_env <- new.env()
result_env$doc1 <- data.frame(summary = "Cached summary", stringsAsFactors = FALSE)
result <- ai_summary(docs, chat_fn = chat_fn, result_env = result_env, verbose = FALSE)
expect_equal(result$summary[1], "Cached summary")
expect_equal(result$summary[2], "Summary of: To be processed.")
})
test_that("ai_summary handles multiple documents", {
llm_output <- readRDS("../test_data/ai_summary_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_1-3.rds")
expect_s3_class(llm_output, "data.frame")
expect_identical(llm_output$id, names(txt))
})
test_that("ai_summary skips already processed documents", {
skip()
chat_fn <- function(...) {
structure(list(
get_model = function() "mock-model",
chat_structured = function(text, type) {
list(summary = paste("Summary of:", text))
}
), class = "MockChat")
}
docs <- c(doc1 = "Already processed.", doc2 = "To be processed.")
result_env <- new.env()
result_env$doc1 <- data.frame(summary = "Cached summary", stringsAsFactors = FALSE)
result <- ai_summary(docs, chat_fn = chat_fn, result_env = result_env, verbose = FALSE)
expect_equal(result$summary[1], "Cached summary")
expect_equal(result$summary[2], "Summary of: To be processed.")
})
make_mock_chat_fn <- function(return_scores = c(0.95, 0.8)) {
i <- 0
function(...) {
structure(list(
get_model = function() "mock-model",
chat_structured = function(text, type) {
i <<- i + 1
list(list(topic = "TopicA", score = return_scores[min(i, length(return_scores))]))
}
), class = "MockChat")
}
}
test_that("ai_salience returns expected output for one document", {
chat_fn <- make_mock_chat_fn(c(0.95))
result <- ai_salience("Test document", topics = c("TopicA"), chat_fn = chat_fn, verbose = FALSE)
expect_s3_class(result, "data.frame")
expect_true("salience_TopicA" %in% colnames(result))
expect_true(is.numeric(result$salience_TopicA))
expect_equal(result$salience_TopicA, 0.95)
})
chat_fn <- make_mock_chat_fn(c(0.95))
result <- ai_salience("Test document", topics = c("TopicA"), chat_fn = chat_fn, verbose = FALSE)
result
chat_fn <- make_mock_chat_fn(c("0.95", "0.8"))
docs <- c("Document 1", "Document 2")
result <- ai_salience(docs, topics = c("TopicA"), chat_fn = chat_fn, verbose = FALSE)
result
expect_s3_class(result, "data.frame")
expect_true(all(c("id", "salience_TopicA") %in% colnames(result)))
expect_equal(nrow(result), length(docs))
expect_type(result$salience_TopicA, "double")
expect_equal(result$salience_TopicA, c(0.95, 0.8))
chat_fn <- make_mock_chat_fn(c("4"))
result <- ai_score("Test document", prompt = "Rate from 1 to 5", chat_fn = chat_fn, verbose = FALSE)
expect_s3_class(result, "data.frame")
prompt <- "Score the following document on a scale of how much it aligns
with the political left. The political left is defined as groups which
advocate for social equality, government intervention in the economy,
and progressive policies. Use the following metrics:
SCORING METRIC:
3 : extremely left
2 : very left
1 : slightly left
0 : not at all left"
scores <- ai_score(data_corpus_inaugural[1:3], prompt,
chat_fn = chat_openai, model = "gpt-4o",
api_args = list(temperature = 0, seed = 42))
View(scores)
docnames(data_corpus_inaugural)
scores <- ai_score(data_corpus_inaugural[57:59], prompt,
chat_fn = chat_openai, model = "gpt-4o",
api_args = list(temperature = 0, seed = 42))
View(scores)
saveRDS(scores, "../test_data/ai_score_example.rds")
?ai_salience
salience <- ai_salience(data_corpus_inaugural[1:3],
chat_fn = chat_openai, model = "gpt-4o",
api_args = list(temperature = 0, seed = 42),
topics = c("economy", "environment", "healthcare"))
?ai+_summary
?ai_summary
library(quanteda)
library(quanteda.llm)
# scoring example
scores <- ai_score(data_corpus_inaugural[57:59], prompt,
chat_fn = chat_openai, model = "gpt-4o",
api_args = list(temperature = 0, seed = 42))
saveRDS(scores, "ai_score_example.rds")
# salience example
salience <- ai_salience(data_corpus_inaugural[1:3],
chat_fn = chat_openai, model = "gpt-4o",
api_args = list(temperature = 0, seed = 42),
topics = c("economy", "environment", "healthcare"))
saveRDS(salience, "ai_salience_example.rds")
# summary example
summaries <- ai_summary(data_corpus_inaugural[1:3],
chat_fn = chat_openai, model = "gpt-4o",
api_args = list(temperature = 0, seed = 42))
saveRDS(summaries, "ai_summary_example.rds")
setwd("~/Dropbox/GitHub/quanteda/quanteda.llm/tests/test_data")
scores <- ai_score(data_corpus_inaugural[57:59], prompt,
chat_fn = chat_openai, model = "gpt-4o",
api_args = list(temperature = 0, seed = 42))
saveRDS(scores, "ai_score_example.rds")
# salience example
salience <- ai_salience(data_corpus_inaugural[1:3],
chat_fn = chat_openai, model = "gpt-4o",
api_args = list(temperature = 0, seed = 42),
topics = c("economy", "environment", "healthcare"))
saveRDS(salience, "ai_salience_example.rds")
# summary example
summaries <- ai_summary(data_corpus_inaugural[1:3],
chat_fn = chat_openai, model = "gpt-4o",
api_args = list(temperature = 0, seed = 42))
saveRDS(summaries, "ai_summary_example.rds")
saveRDS(data_corpus_inaugural[1:3], "data_corpus_inaugural_example_1-3.rds")
saveRDS(data_corpus_inaugural[57:59], "data_corpus_inaugural_example_57_59.rds")
test_that("some test works", {
expect_true(TRUE)
})
test_that("ai_validate throws error for non-character input", {
expect_error(
ai_validate(123, llm_output = list("output"), launch_app = FALSE),
"`text` must be a character vector\\."
)
})
test_that("ai_validate returns expected structure with single text input", {
llm_output <- readRDS("../test_data/ai_summary_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_1-3.rds")
txt <- txt[1]
llm_output <- llm_output[1, ]
result_env <- new.env()
result <- ai_validate(
text = txt,
llm_output = llm_output,
result_env = result_env,
verbose = FALSE,
launch_app = FALSE
)
expect_s3_class(result, "data.frame")
expect_equal(nrow(result), 1)
expect_named(result, c("comments", "examples", "status"))
expect_equal(result$comments, rep("N/A", 1))
expect_equal(result$examples, rep("", 1))
expect_equal(result$status, rep("Unmarked", 1))
})
test_that("ai_validate handles multiple texts input correctly - summary", {
llm_output <- readRDS("../test_data/ai_summary_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_1-3.rds")
result_env <- new.env()
result <- ai_validate(
text = txt,
llm_output = llm_output,
result_env = result_env,
verbose = FALSE,
launch_app = FALSE
)
expect_s3_class(result, "data.frame")
expect_equal(nrow(result), length(txt))
expect_named(result, c("comments", "examples", "status"))
expect_equal(result$comments, rep("N/A", length(txt)))
expect_equal(result$examples, rep("", length(txt)))
expect_equal(result$status, rep("Unmarked", length(txt)))
})
test_that("ai_validate handles multiple texts input correctly - salience", {
llm_output <- readRDS("../test_data/ai_salience_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_1-3.rds")
result_env <- new.env()
result <- ai_validate(
text = txt,
llm_output = llm_output,
result_env = result_env,
verbose = FALSE,
launch_app = FALSE
)
expect_s3_class(result, "data.frame")
expect_equal(nrow(result), length(txt))
expect_named(result, c("comments", "examples", "status"))
expect_equal(result$comments, rep("N/A", length(txt)))
expect_equal(result$examples, rep("", length(txt)))
expect_equal(result$status, rep("Unmarked", length(txt)))
})
test_that("ai_text makes a simple structured request", {
chat <- chat_test("Always return 0.8")
result <- ai_text(
.data = c(doc1 = "What is 1 + 1?"),
chat_fn = chat_test,
type_object = "classification"
)
expect_s3_class(result, "data.frame")
expect_named(result, c("id", "score"))
expect_equal(result$score, 0.8)
})
chat_test <- function(...) {
structure(
list(
get_model = function() "mock-model",
chat_structured = function(text, type) {
list(score = 0.8)
}
),
class = "MockChat"
)
}
test_that("ai_text makes a simple structured request", {
chat <- chat_test("Always return 0.8")
result <- ai_text(
.data = c(doc1 = "What is 1 + 1?"),
chat_fn = chat_test,
type_object = "classification"
)
expect_s3_class(result, "data.frame")
expect_named(result, c("id", "score"))
expect_equal(result$score, 0.8)
})
test_that("ai_text supports few-shot examples", {
chat <- chat_test("Return fixed score with few-shot examples")
few_shot <- data.frame(
text = c("Good text", "Bad text"),
score = c(1, 0)
)
result <- ai_text(
.data = c(doc1 = "Example text"),
chat_fn = chat_test,
type_object = "classification",
few_shot_examples = few_shot
)
expect_equal(result$score, 0.8)
})
test_that("ai_text handles multiple documents", {
chat <- chat_test("Always return 0.8")
result <- ai_text(
.data = c(doc1 = "Text A", doc2 = "Text B"),
chat_fn = chat_test,
type_object = "classification"
)
expect_equal(nrow(result), 2)
expect_equal(result$score, c(0.8, 0.8))
})
test_that("ai_text skips already processed documents", {
chat <- chat_test("Always return 0.8")
env <- new.env()
env[["doc1"]] <- data.frame(score = 1)
result <- ai_text(
.data = c(doc1 = "Old", doc2 = "New"),
chat_fn = chat_test,
type_object = "classification",
result_env = env
)
expect_equal(nrow(result), 2)
expect_equal(result$score, c(1, 0.8))
})
test_that("ai_text errors with non-character input", {
expect_error(
ai_text(.data = 123, chat_fn = chat_test(), type_object = "classification"),
"Unsupported data type"
)
})
test_that("mock chat provider returns correct structure", {
chat <- chat_test()
expect_equal(chat$get_model(), "mock-model")
resp <- chat$chat_structured("Test", type = "classification")
expect_equal(resp, list(score = 0.8))
})
test_that("ai_summary handles multiple documents", {
llm_output <- readRDS("../test_data/ai_summary_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_1-3.rds")
expect_s3_class(llm_output, "data.frame")
expect_identical(llm_output$id, names(txt))
})
test_that("ai_summary skips already processed documents", {
skip()
chat_fn <- function(...) {
structure(list(
get_model = function() "mock-model",
chat_structured = function(text, type) {
list(summary = paste("Summary of:", text))
}
), class = "MockChat")
}
docs <- c(doc1 = "Already processed.", doc2 = "To be processed.")
result_env <- new.env()
result_env$doc1 <- data.frame(summary = "Cached summary", stringsAsFactors = FALSE)
result <- ai_summary(docs, chat_fn = chat_fn, result_env = result_env, verbose = FALSE)
expect_equal(result$summary[1], "Cached summary")
expect_equal(result$summary[2], "Summary of: To be processed.")
})
make_mock_chat_fn <- function(return_scores = c(0.95, 0.8)) {
i <- 0
function(...) {
structure(list(
get_model = function() "mock-model",
chat_structured = function(text, type) {
i <<- i + 1
list(list(topic = "TopicA", score = return_scores[min(i, length(return_scores))]))
}
), class = "MockChat")
}
}
test_that("ai_salience returns expected output for one document", {
chat_fn <- make_mock_chat_fn(c(0.95))
result <- ai_salience("Test document", topics = c("TopicA"), chat_fn = chat_fn, verbose = FALSE)
expect_s3_class(result, "data.frame")
expect_true("salience_TopicA" %in% colnames(result))
expect_true(is.numeric(result$salience_TopicA))
expect_equal(result$salience_TopicA, 0.95)
})
llm_output <- readRDS("../test_data/ai_summary_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_1-3.rds")
txt <- txt[1]
llm_output <- llm_output[1, ]
expect_s3_class(llm_output, "data.frame")
colnames(result)
llm_output
llm_output <- readRDS("../test_data/ai_salience_example.rds")
llm_output
colnames(result)
colnames(llm_output)
dput(colnames(llm_output))
expect_s3_class(llm_output, "data.frame")
expect_identical(
"salience_TopicA" %in%
c("id", "salience_economy", "salience_environment", "salience_healthcare")
)
expect_identical(
c("id", "salience_economy", "salience_environment", "salience_healthcare"),
colnames(llm_output)
)
expect_true(is.numeric(result$salience_economy))
is.numeric(result$salience_economy)
expect_true(is.numeric(llm_output$salience_economy))
llm_output
test_that("ai_salience returns expected output for one document", {
llm_output <- readRDS("../test_data/ai_salience_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_1-3.rds")
expect_s3_class(llm_output, "data.frame")
expect_identical(
c("id", "salience_economy", "salience_environment", "salience_healthcare"),
colnames(llm_output)
)
expect_true(is.numeric(llm_output$salience_economy))
expect_identical(
llm_output$id,
names(txt)
)
})
test_that("all salience values are between 0 and 1", {
llm_output <- readRDS("../test_data/ai_salience_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_1-3.rds")
# Get all salience columns
salience_cols <- grep("^salience_", names(llm_output), value = TRUE)
# Check each column
for (col in salience_cols) {
expect_true(all(llm_output[[col]] >= 0 & llm_output[[col]] <= 1),
info = paste("Values in", col, "should be between 0 and 1"))
}
})
chat_fn <- make_mock_chat_fn(c("4"))
result <- ai_score("Test document", prompt = "Rate from 1 to 5", chat_fn = chat_fn, verbose = FALSE)
expect_s3_class(result, "data.frame")
expect_s3_class(llm_output, "data.frame")
llm_output <- readRDS("../test_data/ai_score_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_57_59.rds")
expect_s3_class(llm_output, "data.frame")
expect_true(all(c("score", "evidence") %in% colnames(result)))
llm_output
View(llm_output)
expect_true(all(c("id", "score", "evidence") %in% colnames(llm_output)))
expect_identical(
c("id", "score", "evidence"),
colnames(llm_output))
result$score
llm_output <- readRDS("../test_data/ai_score_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_57_59.rds")
llm_output <- readRDS("../test_data/ai_score_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_57_59.rds")
llm_output
View(llm_output)
# Get all salience columns
score_cols <- grep("^salience_", names(llm_output), value = TRUE)
score_cols
# Get all salience columns
score_cols <- grep("^score_", names(llm_output), value = TRUE)
score_cols
expect_true(llm_output$score >= 0 & llm_output$score <= 3)
llm_output <- readRDS("../test_data/ai_score_example.rds")
txt <- readRDS("../test_data/data_corpus_inaugural_57_59.rds")
expect_true(all(llm_output$score >= 0 & llm_output$score <= 3))
View(llm_output)
llm_output$score
llm_output$score >= 0
expect_true(all(llm_output$score >= 0 & llm_output$score <= 3))
pkgdown::build_site()
setwd("~/Dropbox/GitHub/quanteda/quanteda.llm")
savehistory("test_history.Rhistory")
# Check your working directory
getwd()
# Look for .Rhistory file
list.files(all.files = TRUE, pattern = "Rhistory")
savehistory("backup_history.Rhistory")
